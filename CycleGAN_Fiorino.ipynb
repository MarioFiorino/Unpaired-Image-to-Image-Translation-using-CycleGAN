{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_2020_Mario_Fiorino_1871233_ok.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvBN7HwHjdZ3",
        "colab_type": "text"
      },
      "source": [
        "**Mario Fiorino 1871233 - Project NN 2020**\n",
        "\n",
        "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n",
        "\n",
        "Paper : https://arxiv.org/abs/1703.10593\n",
        "\n",
        "Original implementations : https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "Bibliographical references for TensorFlow : Nishant Shukla - Machine Learning with TensorFlow\n",
        "\n",
        "# Introduction to Generative Adversarial Networks (GAN)\n",
        "\n",
        "Within the machine learning, the GAN is certainly the most interesting idea of the last 10 years, which has shown remarkable results and a wide versatility. Basically, Generative Adversarial Networks consist in two networks, called generator and discriminator: generator tries to riproduce samples from the true data distribution, while the discriminators try to classify a sample, if it comes from true data distribution or is produced by the generator. The ottimal effect of this is: the generator learns to approximate the true distribution completely, and the discriminator is not able to distinguish between the two distribution, so it is left guessing randomly.\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX2N3rFyv18L",
        "colab_type": "text"
      },
      "source": [
        "# Image-to-image translation and the problem of paired data\n",
        "\n",
        "Image-to-image translation is a class of computer vision , its goal is to learn how transform an image from one representation into another, preserving the semantic of input. Many problems in computer vision can be posed as a translation problem: style transfer,colorizing, semantic segmentation,photo enhancement....\n",
        "\n",
        "So far the most interesting and promising results obtained in this field come from the application of generative adversarial networks. One of the first works that has used this type of approach was Pix2Pix ( presented by Phillip Isola 2016 https://arxiv.org/abs/1611.07004 ). The model works in this way : the generator receives in  input image A, and  translate it in the domain B, producing \"gen_B\" . The discriminator is fed with two input: (A, gen_B), and yields the probability that, given A, \"gen_B\" is the real mapping of A in the domain B . \n",
        "\n",
        "Pix2Pix has produced very good results, but each image from the  first domain must have a corresponding image in the second domain: the pair (A,B). So here the challenge is in training dataset : a paired training dataset is often more difficult and expensive to obtain than an unpaired one, and for many tasks, it is just not feasible! This is where the CycleGAN comes in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnpPTfNJv_Si",
        "colab_type": "text"
      },
      "source": [
        "# Purpose and key idea of CycleGAN\n",
        "\n",
        "\n",
        "This paper, exploiting the adversarial training, present a method that take the characteristics of one image domain and translated these into another image domain (preserving the semantic of input), all in the absence of any paired training examples.\n",
        "\n",
        "Basically the model consists:\n",
        "\n",
        "â€“ two generators: G_AtoB and G_BtoA to translate images of two domain, A â†’ B and vice versa.\n",
        "\n",
        "â€“ two discriminators: D_A and D_B that learns to differentiate between real image to generated image.\n",
        "\n",
        "The key idea is: given in input the image A , applying the generator G_AtoB that generate the image: \" G_AtoB(A) \". Pass this on second generator G_BtoA that yield the image: \"G_BtoA(G_AtoB(A))\" , that is expected to be similar to input A. Schematically:\n",
        "\n",
        "A -> G_AtoB(A) -> G_BtoA (G_AtoB(A)) â‰ˆ A\n",
        "\n",
        "B -> G_BtoA (B) -> G_AtoB(G_BtoA (B)) â‰ˆ B\n",
        "\n",
        "A sort of language translation: translating a sentence from Italian to French, and then translates it back, the resulting is expected to be very close to original sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v95zw9AQTk9C",
        "colab_type": "code",
        "outputId": "e8a51b3c-e333-4512-fe34-e725f2f423e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# Import TensorFlow and other libraries\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import collections\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from imageio import imread\n",
        "from PIL import Image\n",
        "import os \n",
        "from functools import partial\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cEVbi3_ToAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vbe-iRwTtJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation function\n",
        "def relu(x):\n",
        "    return tf.nn.relu(features=x) # rectif : R -> max(O,x)\n",
        "def leakyRelu(x):\n",
        "    return tf.nn.leaky_relu(features=x) # leaky rectif : R -> max(0.1x,x)\n",
        "def tanh(x):\n",
        "    return tf.nn.tanh(x=x)  # tanh : R -> [-1,1]\n",
        " \n",
        "    \n",
        "# Add\n",
        "def add(x,y):\n",
        "    return tf.add(x,y)\n",
        "\n",
        "\n",
        "# Convolution and De-covoluton\n",
        "def conv2d(x, filter, kernel, stride, padding):\n",
        "    return tf.layers.conv2d(inputs=x, filters=filter, kernel_size=kernel, strides=stride, padding=padding) \n",
        "def conv2dTranspose(x, filter, kernel, stride, padding):\n",
        "    return tf.layers.conv2d_transpose(inputs=x, filters=filter, kernel_size=kernel, strides=stride, padding=padding, use_bias=False)\n",
        "\n",
        "\n",
        "# Normalization\n",
        "# The basic idea behind the normalization is to limit internal covariate shift \n",
        "# This, allows each layer to learn on a more stable distribution of inputs, and accelerate the training of the network. A recent paper claims \n",
        "# it is not effective because it reduces internal covariate shift, but because it makes the error function more smooth.\n",
        "def batchNormalization(x):\n",
        "    return tf.layers.batch_normalization(inputs=x, axis=3, momentum=0.9, epsilon=1e-5)\n",
        "#Instance normalization : works similar to batch normalization, but the parameters (mean and variance) are calculated for the single channel, rather than for the entire batch.\n",
        "def instanceNormalization(x):\n",
        "    return tf.contrib.layers.instance_norm(inputs=x, center=True, scale=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miEgPBpBUGH1",
        "colab_type": "text"
      },
      "source": [
        "#  Discriminatorâ€™s network \n",
        "---\n",
        "\n",
        "Input: image  (256,256, 3) -> output: label image real/generated (32,32, 1)\n",
        "\n",
        "----\n",
        "           \n",
        "\n",
        "Convolutional Neural Networks \n",
        "\n",
        "The  discriminator is classically a convolutional network that categorize the images fed to it.\n",
        "A convolutional layer uses small(compared to the input) filters , or also called kernel matrix, which are convolved with the image. For each convolutional step,the dot product of the kernel with the overlapping part of the image is computed. The value â€‹â€‹obtained goes into the feature map of the filter. Performing multiple convolutions on an input, each using a different filter, producing multiple and different feature maps. The weights of the filter are parameters which are learned while training.\n",
        "\n",
        " \n",
        "\n",
        "A mathematical formulation of 2D convolution is ginven:\n",
        "\n",
        "input image: x[i, j]\n",
        "\n",
        "kernel: w[i, j]\n",
        "\n",
        "output convolution  :  âˆ‘_k1âˆ‘_k2( w[k1, k2] Â· x[i - k1, j - k2] ) \n",
        "\n",
        "Note:\n",
        "The area on input where the convolution operation takes place is called the receptive field.\n",
        "\n",
        "_\n",
        "\n",
        "PatchGAN, the model used\n",
        "\n",
        "Instead of classifying the entire image, PatchGAN\n",
        "ranks if N Ã— N portion of the input image are real or not.This is\n",
        "done by stacking convolutional layers after each other, producing a prediction matrix, such that every value of the output has a receptive field of N Ã— N. Each of these values maps to a different NÃ—N patch of the input image. \n",
        "\n",
        "Note :  patchGAN discriminators is fully convolutional, so to process images of different sizes.\n",
        "\n",
        "*\n",
        "\n",
        "Basically the network works in this way:\n",
        "\n",
        "Extract features from the image: \n",
        "\n",
        "\n",
        "*   1 block :  Convolut -> LeakyReLU \n",
        "*   3 hidden block : Convolut -> Instance normaliz -> LeakyReLU \n",
        "\n",
        "Decisional output: \n",
        "\n",
        "*   last block :  Convolut \n",
        "\n",
        "-\n",
        "\n",
        "Layers : [Number of filters x  size] stride\n",
        "\n",
        "* Convol:   [64x4x4]s2 -> [128x4x4]s2 ->[256x4x4]s2 ->[512x4x4]s1 -> [1x4 x4]s1\n",
        "\n",
        "Total Trainable params  â‰…  6.9 Â· 10^6\n",
        "\n",
        "-\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw4zBXHgWvsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator(image,name): \n",
        "       \n",
        "    with tf.variable_scope(name+ '_discriminator', reuse=tf.AUTO_REUSE ):\n",
        "        # 1st Convolutional block\n",
        "        l_conv1 = leakyRelu(conv2d(image, 64, 4, 2, \"same\")) # input = image \n",
        "                                                             # number of filters = 64\n",
        "                                                             # size kernel 4x4\n",
        "                                                             # stride = 2 is the number of pixels by which the filter moves across the image_shifts every time. \n",
        "                                                             # â€œsameâ€ padding specifies that the output size should be the same as the input size; , there is a one-pixel-width padding around the image, and the filter slides outside the image into this padding area\n",
        "              \n",
        "        # 3 Hidden Convolution blocks\n",
        "        hidd_conv1 = leakyRelu(instanceNormalization(conv2d(l_conv1, 128, 4, 2, \"same\")))\n",
        "        hidd_conv2 = leakyRelu(instanceNormalization(conv2d(hidd_conv1, 256, 4, 2, \"same\"))) \n",
        "        hidd_conv3 = leakyRelu(instanceNormalization(conv2d(hidd_conv2, 512, 4, 1, \"same\")))\n",
        "               \n",
        "        # Last layer\n",
        "        output = conv2d(hidd_conv3, 1, 4, 1, \"same\")\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzDuKiw0WwSt",
        "colab_type": "text"
      },
      "source": [
        "# Generator's network \n",
        "-----\n",
        "\n",
        "Input: image in domain (256,256, 3) -> output: image in opposite domain (256,256, 3)\n",
        "\n",
        "----\n",
        "The generator is trained to generate data according to the true data distribution of the training data set. In the field of artificial neural networks , there are several different models which are capable of such tasks, e.g. restricted Boltzmann machines, variational autoencoders,auto-regressive networks,...\n",
        "\n",
        "Due to the peculiarity of the goal of paper, it was used an autoencoder that has special internal structure: it map\n",
        "an image to itself via an intermediate representation that\n",
        "is a translation of the image into another domain. The authors write \" ... can also be seen as a special case of adversarial autoencoders, which use an adversarial loss to train the\n",
        "bottleneck layer of an autoencoder to match an arbitrary target distribution. In our case, the target distribution for the\n",
        "X â†’ X autoencoder is that of the domain Y \".\n",
        "\n",
        "To be clear, the model uses a sequence of downsampling convolutional blocks to encode the input image, a number of residual network convolutional blocks to transform the image ( translation of the image into another domain), and a number of upsampling convolutional blocks to generate the output image.\n",
        "\n",
        "\n",
        "*\n",
        "\n",
        "\n",
        "Basically the network works in this way:\n",
        "\n",
        "*Encoder*\n",
        "\n",
        "*  3 block : Convolut -> Instance normaliz -> ReLU\n",
        "\n",
        "*Residul Block*\n",
        "\n",
        "*   use 9 residual blocks  (for 256x256 pixel training images)\n",
        "\n",
        "\n",
        "*Decoder*\n",
        "\n",
        "*  2 block :  Transposed Conv -> Instance normaliz  -> ReLU \n",
        "*  last one :  Convolut -> Tang_hyper\n",
        "\n",
        "\n",
        "-\n",
        "\n",
        "Layesr : [Number of filters x Kernel size] stride\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Convol: [64x7x7]s1 -> [128x3x3]s2 -> [256x3x3]s2\n",
        "\n",
        "* 9 ( Res-Block : [256x3x3]s1-> [256x3x3]s1 )\n",
        "\n",
        "* De-convol: [128x3x3]s2 -> [64x3x3]s2 -> Convolution: [3x7x7]s1\n",
        "\n",
        "Total Trainable params  â‰… 35 Â· 10^6 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtQHr33cJjyK",
        "colab_type": "text"
      },
      "source": [
        "# Residual network\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "Input: feature vector/encoding (64,64,256) -> output: feature vector/encoding (64,64,256)\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "The basic concept of residual block:\n",
        "\n",
        "Processing an input ð‘¥ in the sequence : \" Convol ->ReLU -> Convol \" obtaining a certain F(x), and then add this result to the same x. So the final output is H(x) = F(x) + x.  (in this way the output do â€œnot change muchâ€ from original input).\n",
        "\n",
        "NOTE: In a traditional forward CNN instead would have H(x) = F(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTcYRQwrI6-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(x): \n",
        "    \n",
        "    res = tf.pad(x, [ [0, 0], [1, 1], [1, 1], [0, 0] ], \"REFLECT\")  # Reflection padding was used to reduce artifacts.\n",
        "    res = relu( batchNormalization( conv2d(res, 256, 3, 1, \"valid\") ) )\n",
        "    res = tf.pad(res, [ [0, 0], [1, 1], [1, 1], [0, 0] ], \"REFLECT\" )\n",
        "    res = batchNormalization( conv2d(res, 256, 3, 1, \"valid\") )\n",
        "    return add(res, x)\n",
        "\n",
        "\n",
        "\n",
        "def build_generator(image, name): \n",
        "  \n",
        "    with tf.variable_scope(name+'_generator', reuse=tf.AUTO_REUSE):\n",
        "        input = tf.pad(image, [[0, 0], [3, 3], [3, 3], [0, 0]], \"CONSTANT\")  \n",
        "    \n",
        "        # Downsampling\n",
        "        h_conv1 = relu( instanceNormalization( conv2d(input, 64, 7, 1, 'valid') ) ) \n",
        "        h_conv2 = relu( instanceNormalization( conv2d(h_conv1, 128, 3, 2, 'same') ) )\n",
        "        h_conv3 = relu( instanceNormalization( conv2d(h_conv2, 256, 3, 2,'same') ) )\n",
        "        \n",
        "        # Residual blocks\n",
        "        residual1 = residual_block(h_conv3)\n",
        "        residual2 = residual_block(residual1)\n",
        "        residual3 = residual_block(residual2)\n",
        "        residual4 = residual_block(residual3)\n",
        "        residual5 = residual_block(residual4)\n",
        "        residual6 = residual_block(residual5)\n",
        "        residual7 = residual_block(residual6)\n",
        "        residual8 = residual_block(residual7)\n",
        "        residual9 = residual_block(residual8)\n",
        "           \n",
        "        # Upsampling blocks\n",
        "        h_deconv4 = relu( instanceNormalization( conv2dTranspose(residual9, 128, 3, 2, 'same') ) ) \n",
        "                                               # Conv2dTransp - namely : deconvolution...\n",
        "        h_deconv5 = relu( instanceNormalization( conv2dTranspose(h_deconv4, 64, 3, 2, 'same') ) )\n",
        "       \n",
        "\n",
        "        # Last Convolution layer\n",
        "        output = tanh( conv2d(h_deconv5, 3, 7, 1, 'same') )  # Reconstruction image opposite domain\n",
        "        \n",
        "        \n",
        "        return output "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMZMRGcHWol5",
        "colab_type": "text"
      },
      "source": [
        "# Load data and image augmentation procedure\n",
        "\n",
        "The images are fished randomly ( by uniform distribution) from the respective dataset folders. Then is applied a procedure for image augmentation: \n",
        "\n",
        "random cropping (50% probability to apply) \n",
        "\n",
        "flipping left/right(50% probability to apply).\n",
        "\n",
        " \n",
        "\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc_GaItoEgsC",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "â€¢\tThe dataset uses for experimental phase is Â«vangogh2photoÂ».\n",
        " Downloaded from: people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets.\n",
        "\n",
        "â€¢\tContent RGB images of Van Gogh paintings and photo of different landscape in different angulation, lights, contexts.\n",
        " With resolution 256 x 256 pixels. \n",
        "\n",
        "â€¢\tThe training set size of each class: 400 (Van Gogh) and 6287(landscape).\n",
        "\n",
        "â€¢\tTest set size : 400 (VG) and 750 (land)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX1RH8uwDJIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Proc_image(data_dir,imagesA,imagesB):\n",
        "\n",
        "    A = np.random.choice(imagesA)  # randomly from dataset A\n",
        "    B = np.random.choice(imagesB)  # randomly from dataset B\n",
        "  \n",
        "    A = np.array(Image.fromarray(imread(A, pilmode='RGB')).resize((256, 256)))   \n",
        "    B = np.array(Image.fromarray(imread(B, pilmode='RGB')).resize((256, 256)))\n",
        "\n",
        "    \n",
        "    # Cropped  A\n",
        "    if np.random.random() > 0.5:\n",
        "         dc =  random.uniform(0.0, 0.2) \n",
        "         crop = iaa.Crop(percent=(0, dc)) \n",
        "         A = crop.augment_image(A)\n",
        "    \n",
        "    # Flip in the left/right direction \n",
        "    if np.random.random() > 0.5:\n",
        "        A = np.fliplr(A) \n",
        "\n",
        "    #Cropped B\n",
        "    if np.random.random() > 0.5:\n",
        "         dc =  random.uniform(0.0, 0.1)\n",
        "         crop = iaa.Crop(percent=(0, dc)) \n",
        "         B = crop.augment_image(B)\n",
        "    \n",
        "    # Flip in the left/right direction \n",
        "    if np.random.random() > 0.5:\n",
        "        B = np.fliplr(B)     \n",
        "\n",
        "    # Normaliz\n",
        "    A =  (A/ 127.5) - 1.0\n",
        "    B =  (B/ 127.5) - 1.0\n",
        "     \n",
        "    return A , B \n",
        "\n",
        "\n",
        "# Save training summary (Tensorboard)\n",
        "def Train_summary(writer,T_name, value, global_step): \n",
        "    summary = tf.Summary(value=[tf.Summary.Value(tag=T_name, simple_value=value)])\n",
        "    writer.add_summary(summary, global_step=global_step)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb5ZRcYWRxAc",
        "colab_type": "text"
      },
      "source": [
        "# Loss functions\n",
        "\n",
        "\n",
        "Discriminator adversarial loss \n",
        "\n",
        "The discriminator must be trained in such a way as to recognize the original images and reject those generated. \n",
        "So, to be clearer, taken the case of the discriminator D_A  ( the same procedure goes for D_B  as well ), showing with \"D_A(image input)\" his output, the operations for train the network :\n",
        "\n",
        "1.\tminimize   *E* [(D_A (A) â€“ 1)^2] ; namely : given in input the original image A the discriminator should recognize it ,getting a value close to 1 (means \" true\"). \n",
        "\n",
        "2.\tminimize   *E* [( D_A (G_BtoA (B) )^2] ; namely :  the discriminator should predict 0 (\" false\") for the \" generated images A \" produced by the generator G_BtoA.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bw_BSV8T0ZI",
        "colab_type": "text"
      },
      "source": [
        "Generator adversarial loss \n",
        "\n",
        "Generator should eventually be able to fool the discriminator about the authenticity of it's generated images. If the generator is performing well, the discriminator will classify the fake images as real (or 1). So:\n",
        "\n",
        "\n",
        "1. minimize  *E* [(D_A (G_BtoA (B) - 1 )^2] ; namely: evaluation of discriminator for the generated image \"G_BtoA (B)\" is as close as possible to 1 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEVrhggpUf9J",
        "colab_type": "text"
      },
      "source": [
        "The two procedures above are typical of the GAN;  the one that follows, instead, characterizes the cycleGAN ; ***to define a meaningful mapping ,in absent of unpaired dataset, the authors introduce the constraint of cycle consistency.***\n",
        "\n",
        "Cyclic loss:\n",
        "1.\tminimize  *E* [ | G_AtoB(G_BtoA (B)) â€“ B | ] + *E* [ | G_BtoA (G_AtoB(A)) â€“ A | ] ; namely : the difference between the original image and the cyclic generate  image should be as small as possible\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "\n",
        "Identity loss:\n",
        "\n",
        "Identity loss says that, if you fed image B to generator G_AtoB , it should yield the real image B  or something close to image B. \n",
        "\n",
        "1.   minimize  *E* [ | G_AtoB(B) â€“ B | ] + *E* [ | G_BtoA(A) â€“ A | ]\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "Note, have been used : \n",
        "\n",
        "Mean squared error for the scalars (discriminator probabilities)\n",
        "\n",
        "Mean absolute error for images ( reconstructed and identity-mapped)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1WpPGuhCdd8",
        "colab_type": "text"
      },
      "source": [
        "# Optimization algorithm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The method of gradient descent is used to change the weights in the direction of the minimum of the loss function. To apply gradient descent we need the gradient of the loss function with respect\n",
        "to the weights of every layer: dE/dWn( in simple word : We want to know how much a change in Wn affects the total error).The backpropagation algorithm uses the chain\n",
        "rule to compute this gradient. The chain rule is a formula to compute the derivative of a composite function; simplifying the process : derivative of the error wrt the activation * derivative of the activation wrt net input * derivative of the net input with respect to  weight n. By this way, the weights of each layer can be update  reducing,gradually ,total error. \n",
        " Easy update equation : Wn = Wn - learning rate * dE/dWn .\n",
        "\n",
        "*\n",
        "\n",
        "\n",
        "Adam (Adaptive moment estimation)\n",
        "\n",
        "\" Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods\". Kingma, Ba\n",
        "\n",
        "In my code ,the process of optimizing of the neural network uses algorithm Adam, it combines the advantages of two stochastic gradient descent: AdaGrad and RMSProp.\n",
        "\n",
        "AdaGrad adapt the learning rate at each time step for every parameter, based on the gradient of the previous steps ( it is well-suited for sparse data):\n",
        "\n",
        "Wn = Wn - ( lr /  âˆš G + Îµ ) * g\n",
        "\n",
        "For the readable of the formula,  using notation g for gradient dE/dWn at current time step. \n",
        "\n",
        "\n",
        "G is the sum of the squares of the gradients wrt Wn up to the present time step. Note : this factor attenuates the learning rate of parameters subject to abrupt updates, and gives greater importance to the learning rate of parameters with infrequent updates.\n",
        "\n",
        "Îµ is a smoothing term that avoids division by zero, usually on the order of 10^âˆ’8\n",
        "\n",
        "In AdaGrad, since G is positive, it continues to grow with each iteration, and the learning rate decreases to 0, leading to an arrest of learning. The solution proposed for this problem, in RMSProp,  was redefine G as an exponential moving average at time t :\n",
        "\n",
        " E[g^2]_t =  Î³ E [ g^2]_t-1  +  (1-Î³) g^2.\n",
        "\n",
        "\n",
        "Î³ determines the importance  of past iterations in calculating the moving average: a value cloe to zero means that past iterations are ignored and only the gradient to the current iteration is used\n",
        "\n",
        "\n",
        "Adam, in addition to storing an exponentially decaying average of past squared gradients E[g^2],  also keeps an exponentially decaying average of past gradients E[g]. To be precise:\n",
        "\n",
        "m_t = E[g]_t   = Î²1 * m_t-1 + (1-Î²1)g\n",
        "\n",
        "v_t = E[g^2]_t =  Î²2 * v_t-1 + (1-Î²2)g^2\n",
        "\n",
        "m_t and v_t are estimates of the first moment and the second moment of the gradients. Since both are initialized to 0, their values tend to remain very small, especially in the early stages of training; therefore,the correct estimators  are :\n",
        "\n",
        "m_t = m_t / 1 - Î²1\n",
        "\n",
        "v_t = v_t / 1 - Î²2\n",
        "\n",
        "Adam update rule :  \n",
        "Wn = Wn - (lr / âˆš v_t + Îµ ) * m_t\n",
        "\n",
        "*\n",
        "\n",
        "In Tensorflow for the training op to minimize the loss function it is possible to use a predefined Optimizer : tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMvhl8ONRoXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CycleGAN(): \n",
        "\n",
        "    \n",
        "    generatorAToB = partial(build_generator, name='AToB')\n",
        "    generatorBToA = partial(build_generator, name='BToA')\n",
        "        \n",
        "    discriminatorA = partial(build_discriminator, name='A')\n",
        "    discriminatorB = partial(build_discriminator, name='B')\n",
        "\n",
        "\n",
        "    # Define input in flow graph    \n",
        "    real_imageA = tf.placeholder(\"float\", shape=[None, 256, 256, 3], name=\"Image_real_A\")    \n",
        "    real_imageB = tf.placeholder(\"float\", shape=[None, 256, 256, 3], name=\"Image_real_B\")\n",
        "    \n",
        "    # Generated images using both of the generator networks\n",
        "    simulate_imageA = generatorBToA(real_imageB)\n",
        "    simulate_imageB = generatorAToB(real_imageA)\n",
        "    \n",
        "    # Reconstruct images back to original images\n",
        "    reconstructedA = generatorBToA(simulate_imageB)\n",
        "    reconstructedB = generatorAToB(simulate_imageA)\n",
        "    \n",
        "    # Same generated for identity loss\n",
        "    sameB_gen = generatorAToB(real_imageB)\n",
        "    sameA_gen = generatorBToA(real_imageA)\n",
        "    \n",
        "     # Discriminator\n",
        "    decision_r_A = discriminatorA(real_imageA)  \n",
        "    decision_s_A = discriminatorA(simulate_imageA)\n",
        "\n",
        "    decision_r_B = discriminatorB(real_imageB)\n",
        "    decision_s_B = discriminatorB(simulate_imageB)\n",
        "\n",
        " \n",
        "\n",
        "    with tf.variable_scope(\"LossDiscriminatorA\"):\n",
        "        dA_loss_real = tf.losses.mean_squared_error(labels= decision_r_A, predictions=tf.ones_like(decision_r_A)) \n",
        "        # Note : array of ones,since these are the real images\n",
        "        dA_loss_gBtoA = tf.losses.mean_squared_error(labels= decision_s_A, predictions=tf.zeros_like(decision_s_A))\n",
        "        # Note : array of zeros,since these are the fake images\n",
        "        dA_loss =  (dA_loss_real + dA_loss_gBtoA)   # Total discriminator A\n",
        "       \n",
        "    with tf.variable_scope(\"LossDiscriminatorB\"):\n",
        "        dB_loss_real = tf.losses.mean_squared_error(labels=decision_r_B, predictions= tf.ones_like(decision_r_B))\n",
        "        dB_loss_gAtoB = tf.losses.mean_squared_error(labels=decision_s_B, predictions=tf.zeros_like(decision_s_B))\n",
        "        dB_loss = (dB_loss_real + dB_loss_gAtoB)\n",
        "\n",
        "    with tf.variable_scope('LossCyclic'):    \n",
        "        # Cyclic loss\n",
        "        cyc_loss_A = tf.losses.absolute_difference(real_imageA, reconstructedA)\n",
        "        cyc_loss_B = tf.losses.absolute_difference(real_imageB, reconstructedB) \n",
        "        cyc_loss = cyc_loss_A + cyc_loss_B \n",
        "\n",
        "    with tf.variable_scope('IdentityLoss'):    \n",
        "        # Identity loss\n",
        "        ide_loss_A = tf.losses.absolute_difference(real_imageA, sameA_gen)\n",
        "        ide_loss_B = tf.losses.absolute_difference(real_imageB, sameB_gen)  \n",
        "       \n",
        "             \n",
        "    with tf.variable_scope('LossGeneratorAtoB'):\n",
        "        g_loss_AtoB = tf.losses.mean_squared_error(labels=decision_s_B, predictions=tf.ones_like(decision_s_B))\n",
        "        # Total generator loss  = adversarial loss + cycle loss + identity\n",
        "        gAtoB_loss = g_loss_AtoB + cyc_loss * 10.0 + ide_loss_B * 4.0\n",
        "        # Note : The multiplicative factor of 10 for cyc_loss assigns more importance to cyclic loss, \n",
        "        #        for identity loss is 4 \n",
        "          \n",
        "    with tf.variable_scope('LossGeneratorBtoA'):  \n",
        "        g_loss_BtoA = tf.losses.mean_squared_error(labels=decision_s_A, predictions=tf.ones_like(decision_s_A))\n",
        "        gBtoA_loss  = g_loss_BtoA + cyc_loss * 10.0 + ide_loss_A * 4.0 \n",
        "\n",
        "    with tf.variable_scope(\"Train\"):    \n",
        "        dA_vars = [var for var in tf.trainable_variables() if 'A_discriminator' in var.name]\n",
        "        dB_vars = [var for var in tf.trainable_variables() if 'B_discriminator' in var.name]\n",
        "        gAtoB_vars = [var for var in tf.trainable_variables() if 'AToB_generator' in var.name]\n",
        "        gBtoA_vars = [var for var in tf.trainable_variables() if 'BToA_generator' in var.name]\n",
        "        \n",
        "        adam = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)  \n",
        "        \n",
        "        train_dA = adam.minimize(dA_loss, var_list=dA_vars)  \n",
        "        train_dB = adam.minimize(dB_loss, var_list=dB_vars)\n",
        "        train_gAtoB = adam.minimize(gAtoB_loss, var_list=gAtoB_vars) \n",
        "        train_gBtoA = adam.minimize(gBtoA_loss, var_list=gBtoA_vars)\n",
        "    \n",
        "    return real_imageA, real_imageB, simulate_imageA, simulate_imageB, reconstructedA, reconstructedB, dA_loss, dB_loss, gAtoB_loss, gBtoA_loss, train_dA, train_dB, train_gAtoB, train_gBtoA\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UajGjWM5JU2J",
        "colab_type": "text"
      },
      "source": [
        "# CycleGAN  training algorithm \n",
        "\n",
        "\n",
        "0: Initialisation phase\n",
        "\n",
        "for number of training iterations do:\n",
        "\n",
        "for batch size do:\n",
        "\n",
        "1:  Sample a image from distribution p_data_A\n",
        "\n",
        "2:  Sample a image from distribution p_data_B \n",
        "\n",
        "3: Generate  prediction  :  A â†’  G_AtoB(A)\n",
        " \n",
        "4: Generate  prediction  :  B â†’ G_BtoA(B)\n",
        "\n",
        "5: Generate reconstructed sample A :  G_AtoB(A) â†’  G_BtoA ( G_AtoB(A) )\n",
        "\n",
        "6: Generate reconstructed sample B : G_BtoA(B)  â†’  G_AtoB ( G_BtoA(B) )\n",
        "\n",
        "7: Optimizing G_AtoB network by Adam computing 'LossGeneratorAtoB' : adversarial generator loss + 10.0 cycle loss + 4.0 identity loss  \n",
        "\n",
        "\n",
        "8: Optimizing D_B network by Adam computing  'LossDiscriminatorB' : adversarial discriminator loss\n",
        "\n",
        "\n",
        "9:  Optimizing  G_BtoA network by Adam computing 'LossGeneratorBtoA' : adversarial generator loss + 10.0 cycle loss + 4.0 identity loss\n",
        "\n",
        "10: Optimizing D_A network by Adam computing 'LossDiscriminatorA' : adversarial discriminator loss\n",
        "\n",
        "end for\n",
        "\n",
        "end for. \n",
        "\n",
        "\n",
        "\n",
        "NOTE : Rows from 3 to 6 are opportunely recalculated for loss functions in each Opt phase; and not only once time, as the algorithm scheme may suggest.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjOkvG99JYeH",
        "colab_type": "code",
        "outputId": "1b150488-7841-49d7-8229-e6e8e8ac798c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 701\n",
        "\n",
        "data_dir = \"/content/drive/My Drive/vangogh2photo\"\n",
        "log_dir =  \"/content/drive/My Drive/logs\"\n",
        "model_path = \"/content/drive/My Drive/model/model.ckpt\"\n",
        "meta_graph_path = \"/content/drive/My Drive/meta/model.ckpt.meta\"\n",
        "checkpoint_save_path = \"/content/drive/My Drive/meta/model.ckpt\"\n",
        "\n",
        "\n",
        "#Load dataset training\n",
        "imagesA =  glob(data_dir + '/trainA/*.*')\n",
        "imagesB =  glob(data_dir + '/trainB/*.*')\n",
        "TA =len(imagesA)\n",
        "TB =len(imagesB)\n",
        "print('Number of files in dataset training A and B ',TA,' and ', TB)\n",
        "print('')\n",
        "\n",
        "tf.reset_default_graph()\n",
        "g = tf.Graph()\n",
        "\n",
        "with g.as_default():  \n",
        "    real_imageA, real_imageB, simulate_imageA, simulate_imageB, reconstructedA, reconstructedB, dA_loss, dB_loss, gAtoB_loss, gBtoA_loss, train_dA, train_dB, train_gAtoB, train_gBtoA = CycleGAN()\n",
        "    saver = tf.train.Saver() \n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "     # Restoring process\n",
        "     if tf.train.latest_checkpoint('/content/drive/My Drive/meta/'):\n",
        "        print(\"Restoring model\")\n",
        "        saver.restore(sess, tf.train.latest_checkpoint('/content/drive/My Drive/meta/'))\n",
        "     else :   \n",
        "        print('Initializating')\n",
        "        sess.run(tf.local_variables_initializer())\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "    \n",
        "     train_writer = tf.summary.FileWriter(log_dir, sess.graph)    \n",
        "     print('') \n",
        "\n",
        "     # Number of images processed for epoch\n",
        "     bs = int(TA/4)    #Dataset A : len(imagesA) = 400 painting, so bs = 100 painting processed each epoch   \n",
        "      \n",
        "     for epoch in range(600,epochs):\n",
        "        print(\"Epoch:{}\".format(epoch))\n",
        "\n",
        "        for nb in range(bs):   \n",
        "\n",
        "            # Image A and B random fished by their respective dataset and then processed : image augmentation techniques              \n",
        "            imageA, imageB = Proc_image(data_dir,imagesA,imagesB)\n",
        "                                               \n",
        "\n",
        "            # Optimizing:  \n",
        "            \n",
        "            #G_AtoB network \n",
        "            gAtoB_loss_val, _ = sess.run([gAtoB_loss, train_gAtoB], feed_dict={real_imageA:imageA.reshape(1,256,256,3), real_imageB:imageB.reshape(1,256,256,3)})\n",
        "            \n",
        "            #Discriminator B \n",
        "            d_B_loss_val, _ = sess.run([dB_loss, train_dB], feed_dict={real_imageA:imageA.reshape(1,256,256,3), real_imageB:imageB.reshape(1,256,256,3)})\n",
        "                       \n",
        "            # G_BtoA network\n",
        "            gBtoA_loss_val, _ = sess.run([gBtoA_loss, train_gBtoA], feed_dict={real_imageA:imageA.reshape(1,256,256,3), real_imageB:imageB.reshape(1,256,256,3)})\n",
        "            \n",
        "            #Discriminator A \n",
        "            d_A_loss_val, _ = sess.run([dA_loss, train_dA], feed_dict={real_imageA:imageA.reshape(1,256,256,3), real_imageB:imageB.reshape(1,256,256,3)})\n",
        "            \n",
        "            \n",
        "           \n",
        "        # Save losses to Tensorboard after each epoch\n",
        "        Train_summary(train_writer, \"gAtoB_loss\", gAtoB_loss_val, epoch)\n",
        "        Train_summary(train_writer, \"gBtoA_loss\", gBtoA_loss_val, epoch)\n",
        "        Train_summary(train_writer, \"dA_loss\", d_A_loss_val, epoch)\n",
        "        Train_summary(train_writer, \"dB_loss\", d_B_loss_val, epoch)\n",
        "        \n",
        "        print('gAtoB_loss  ',round(gAtoB_loss_val,2),'      gBtoA_loss  ',round(gBtoA_loss_val,2),'  |   dB_loss  ',round(d_B_loss_val,2),  '      dA_loss   ',round(d_A_loss_val,2))\n",
        "        print('')\n",
        "                      \n",
        "            \n",
        "        # Save after every ... epochs\n",
        "        if epoch % 100 == 0: \n",
        "            save_path = saver.save(sess, checkpoint_save_path)\n",
        "                              \n",
        "     save_path = saver.save(sess, model_path)\n",
        "     train_writer.close() \n",
        "     print()\n",
        "     print('End of training')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of files in dataset training A and B  400  and  6287\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-343a5f7c3b01>:16: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-343a5f7c3b01>:25: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-3-343a5f7c3b01>:18: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Restoring model\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/meta/model.ckpt\n",
            "\n",
            "Epoch:600\n",
            "gAtoB_loss   5.12       gBtoA_loss   5.13   |   dB_loss   0.53       dA_loss    0.35\n",
            "\n",
            "Epoch:601\n",
            "gAtoB_loss   3.82       gBtoA_loss   3.35   |   dB_loss   0.52       dA_loss    0.54\n",
            "\n",
            "Epoch:602\n",
            "gAtoB_loss   3.8       gBtoA_loss   3.84   |   dB_loss   0.31       dA_loss    0.23\n",
            "\n",
            "Epoch:603\n",
            "gAtoB_loss   3.55       gBtoA_loss   3.63   |   dB_loss   0.1       dA_loss    0.1\n",
            "\n",
            "Epoch:604\n",
            "gAtoB_loss   3.19       gBtoA_loss   2.74   |   dB_loss   0.34       dA_loss    0.3\n",
            "\n",
            "Epoch:605\n",
            "gAtoB_loss   4.05       gBtoA_loss   3.74   |   dB_loss   0.4       dA_loss    0.18\n",
            "\n",
            "Epoch:606\n",
            "gAtoB_loss   4.2       gBtoA_loss   4.13   |   dB_loss   0.31       dA_loss    0.22\n",
            "\n",
            "Epoch:607\n",
            "gAtoB_loss   4.8       gBtoA_loss   5.1   |   dB_loss   0.27       dA_loss    0.2\n",
            "\n",
            "Epoch:608\n",
            "gAtoB_loss   4.43       gBtoA_loss   3.96   |   dB_loss   0.5       dA_loss    0.33\n",
            "\n",
            "Epoch:609\n",
            "gAtoB_loss   4.57       gBtoA_loss   4.55   |   dB_loss   0.13       dA_loss    0.08\n",
            "\n",
            "Epoch:610\n",
            "gAtoB_loss   3.1       gBtoA_loss   3.12   |   dB_loss   0.24       dA_loss    0.15\n",
            "\n",
            "Epoch:611\n",
            "gAtoB_loss   4.84       gBtoA_loss   5.18   |   dB_loss   0.14       dA_loss    0.11\n",
            "\n",
            "Epoch:612\n",
            "gAtoB_loss   3.79       gBtoA_loss   3.34   |   dB_loss   0.32       dA_loss    0.42\n",
            "\n",
            "Epoch:613\n",
            "gAtoB_loss   4.62       gBtoA_loss   3.72   |   dB_loss   0.28       dA_loss    0.12\n",
            "\n",
            "Epoch:614\n",
            "gAtoB_loss   4.27       gBtoA_loss   3.77   |   dB_loss   0.5       dA_loss    0.14\n",
            "\n",
            "Epoch:615\n",
            "gAtoB_loss   3.43       gBtoA_loss   3.4   |   dB_loss   0.13       dA_loss    0.36\n",
            "\n",
            "Epoch:616\n",
            "gAtoB_loss   4.07       gBtoA_loss   5.08   |   dB_loss   0.37       dA_loss    0.24\n",
            "\n",
            "Epoch:617\n",
            "gAtoB_loss   4.76       gBtoA_loss   4.04   |   dB_loss   0.28       dA_loss    0.15\n",
            "\n",
            "Epoch:618\n",
            "gAtoB_loss   3.72       gBtoA_loss   3.46   |   dB_loss   0.38       dA_loss    0.58\n",
            "\n",
            "Epoch:619\n",
            "gAtoB_loss   4.04       gBtoA_loss   4.11   |   dB_loss   0.12       dA_loss    0.08\n",
            "\n",
            "Epoch:620\n",
            "gAtoB_loss   4.08       gBtoA_loss   3.87   |   dB_loss   0.1       dA_loss    0.34\n",
            "\n",
            "Epoch:621\n",
            "gAtoB_loss   3.86       gBtoA_loss   4.07   |   dB_loss   0.35       dA_loss    0.2\n",
            "\n",
            "Epoch:622\n",
            "gAtoB_loss   3.16       gBtoA_loss   3.16   |   dB_loss   0.19       dA_loss    0.14\n",
            "\n",
            "Epoch:623\n",
            "gAtoB_loss   4.55       gBtoA_loss   5.2   |   dB_loss   0.23       dA_loss    0.03\n",
            "\n",
            "Epoch:624\n",
            "gAtoB_loss   6.1       gBtoA_loss   5.32   |   dB_loss   0.08       dA_loss    0.16\n",
            "\n",
            "Epoch:625\n",
            "gAtoB_loss   5.61       gBtoA_loss   5.33   |   dB_loss   0.14       dA_loss    0.23\n",
            "\n",
            "Epoch:626\n",
            "gAtoB_loss   4.73       gBtoA_loss   4.92   |   dB_loss   0.34       dA_loss    0.25\n",
            "\n",
            "Epoch:627\n",
            "gAtoB_loss   4.83       gBtoA_loss   4.72   |   dB_loss   0.28       dA_loss    0.08\n",
            "\n",
            "Epoch:628\n",
            "gAtoB_loss   4.99       gBtoA_loss   4.72   |   dB_loss   0.22       dA_loss    0.17\n",
            "\n",
            "Epoch:629\n",
            "gAtoB_loss   4.25       gBtoA_loss   3.96   |   dB_loss   0.4       dA_loss    0.12\n",
            "\n",
            "Epoch:630\n",
            "gAtoB_loss   3.8       gBtoA_loss   3.69   |   dB_loss   0.51       dA_loss    0.18\n",
            "\n",
            "Epoch:631\n",
            "gAtoB_loss   4.13       gBtoA_loss   4.48   |   dB_loss   0.55       dA_loss    0.29\n",
            "\n",
            "Epoch:632\n",
            "gAtoB_loss   3.32       gBtoA_loss   3.9   |   dB_loss   0.61       dA_loss    0.22\n",
            "\n",
            "Epoch:633\n",
            "gAtoB_loss   4.4       gBtoA_loss   4.3   |   dB_loss   0.31       dA_loss    0.16\n",
            "\n",
            "Epoch:634\n",
            "gAtoB_loss   8.71       gBtoA_loss   7.13   |   dB_loss   0.18       dA_loss    0.52\n",
            "\n",
            "Epoch:635\n",
            "gAtoB_loss   3.59       gBtoA_loss   3.29   |   dB_loss   0.31       dA_loss    0.63\n",
            "\n",
            "Epoch:636\n",
            "gAtoB_loss   3.95       gBtoA_loss   4.11   |   dB_loss   0.12       dA_loss    0.1\n",
            "\n",
            "Epoch:637\n",
            "gAtoB_loss   4.25       gBtoA_loss   3.86   |   dB_loss   0.2       dA_loss    0.24\n",
            "\n",
            "Epoch:638\n",
            "gAtoB_loss   3.5       gBtoA_loss   3.29   |   dB_loss   0.55       dA_loss    0.16\n",
            "\n",
            "Epoch:639\n",
            "gAtoB_loss   4.71       gBtoA_loss   4.72   |   dB_loss   0.55       dA_loss    0.11\n",
            "\n",
            "Epoch:640\n",
            "gAtoB_loss   4.55       gBtoA_loss   4.27   |   dB_loss   0.63       dA_loss    0.23\n",
            "\n",
            "Epoch:641\n",
            "gAtoB_loss   4.0       gBtoA_loss   3.66   |   dB_loss   0.55       dA_loss    0.24\n",
            "\n",
            "Epoch:642\n",
            "gAtoB_loss   4.05       gBtoA_loss   4.13   |   dB_loss   0.43       dA_loss    0.23\n",
            "\n",
            "Epoch:643\n",
            "gAtoB_loss   2.92       gBtoA_loss   3.51   |   dB_loss   0.39       dA_loss    0.1\n",
            "\n",
            "Epoch:644\n",
            "gAtoB_loss   4.76       gBtoA_loss   4.77   |   dB_loss   0.22       dA_loss    0.13\n",
            "\n",
            "Epoch:645\n",
            "gAtoB_loss   3.58       gBtoA_loss   3.59   |   dB_loss   0.51       dA_loss    0.09\n",
            "\n",
            "Epoch:646\n",
            "gAtoB_loss   3.77       gBtoA_loss   4.08   |   dB_loss   0.43       dA_loss    0.14\n",
            "\n",
            "Epoch:647\n",
            "gAtoB_loss   4.09       gBtoA_loss   3.93   |   dB_loss   0.73       dA_loss    0.26\n",
            "\n",
            "Epoch:648\n",
            "gAtoB_loss   4.38       gBtoA_loss   4.58   |   dB_loss   0.18       dA_loss    0.06\n",
            "\n",
            "Epoch:649\n",
            "gAtoB_loss   4.75       gBtoA_loss   4.29   |   dB_loss   0.34       dA_loss    0.27\n",
            "\n",
            "Epoch:650\n",
            "gAtoB_loss   3.72       gBtoA_loss   3.63   |   dB_loss   0.11       dA_loss    0.16\n",
            "\n",
            "Epoch:651\n",
            "gAtoB_loss   2.97       gBtoA_loss   3.5   |   dB_loss   0.52       dA_loss    0.06\n",
            "\n",
            "Epoch:652\n",
            "gAtoB_loss   4.14       gBtoA_loss   4.11   |   dB_loss   0.52       dA_loss    0.45\n",
            "\n",
            "Epoch:653\n",
            "gAtoB_loss   4.0       gBtoA_loss   3.9   |   dB_loss   0.14       dA_loss    0.12\n",
            "\n",
            "Epoch:654\n",
            "gAtoB_loss   4.32       gBtoA_loss   3.9   |   dB_loss   0.19       dA_loss    0.43\n",
            "\n",
            "Epoch:655\n",
            "gAtoB_loss   3.94       gBtoA_loss   3.41   |   dB_loss   0.17       dA_loss    0.16\n",
            "\n",
            "Epoch:656\n",
            "gAtoB_loss   4.21       gBtoA_loss   3.15   |   dB_loss   0.56       dA_loss    0.38\n",
            "\n",
            "Epoch:657\n",
            "gAtoB_loss   3.76       gBtoA_loss   3.83   |   dB_loss   0.12       dA_loss    0.14\n",
            "\n",
            "Epoch:658\n",
            "gAtoB_loss   3.13       gBtoA_loss   3.01   |   dB_loss   0.29       dA_loss    0.15\n",
            "\n",
            "Epoch:659\n",
            "gAtoB_loss   4.54       gBtoA_loss   4.49   |   dB_loss   0.29       dA_loss    0.05\n",
            "\n",
            "Epoch:660\n",
            "gAtoB_loss   2.91       gBtoA_loss   3.02   |   dB_loss   0.4       dA_loss    0.11\n",
            "\n",
            "Epoch:661\n",
            "gAtoB_loss   4.06       gBtoA_loss   3.83   |   dB_loss   0.81       dA_loss    0.13\n",
            "\n",
            "Epoch:662\n",
            "gAtoB_loss   4.51       gBtoA_loss   4.86   |   dB_loss   0.21       dA_loss    0.58\n",
            "\n",
            "Epoch:663\n",
            "gAtoB_loss   5.12       gBtoA_loss   4.96   |   dB_loss   0.12       dA_loss    0.16\n",
            "\n",
            "Epoch:664\n",
            "gAtoB_loss   3.03       gBtoA_loss   3.19   |   dB_loss   0.11       dA_loss    0.51\n",
            "\n",
            "Epoch:665\n",
            "gAtoB_loss   3.48       gBtoA_loss   3.88   |   dB_loss   0.51       dA_loss    0.1\n",
            "\n",
            "Epoch:666\n",
            "gAtoB_loss   3.9       gBtoA_loss   3.22   |   dB_loss   0.25       dA_loss    0.36\n",
            "\n",
            "Epoch:667\n",
            "gAtoB_loss   4.4       gBtoA_loss   4.68   |   dB_loss   0.45       dA_loss    0.39\n",
            "\n",
            "Epoch:668\n",
            "gAtoB_loss   3.76       gBtoA_loss   3.86   |   dB_loss   0.17       dA_loss    0.41\n",
            "\n",
            "Epoch:669\n",
            "gAtoB_loss   4.49       gBtoA_loss   3.53   |   dB_loss   0.53       dA_loss    0.42\n",
            "\n",
            "Epoch:670\n",
            "gAtoB_loss   4.19       gBtoA_loss   3.37   |   dB_loss   0.18       dA_loss    0.17\n",
            "\n",
            "Epoch:671\n",
            "gAtoB_loss   4.48       gBtoA_loss   4.33   |   dB_loss   0.5       dA_loss    0.11\n",
            "\n",
            "Epoch:672\n",
            "gAtoB_loss   3.97       gBtoA_loss   3.16   |   dB_loss   0.1       dA_loss    0.17\n",
            "\n",
            "Epoch:673\n",
            "gAtoB_loss   4.03       gBtoA_loss   4.08   |   dB_loss   0.35       dA_loss    0.19\n",
            "\n",
            "Epoch:674\n",
            "gAtoB_loss   3.73       gBtoA_loss   4.14   |   dB_loss   0.74       dA_loss    0.09\n",
            "\n",
            "Epoch:675\n",
            "gAtoB_loss   4.46       gBtoA_loss   4.11   |   dB_loss   0.2       dA_loss    0.17\n",
            "\n",
            "Epoch:676\n",
            "gAtoB_loss   3.59       gBtoA_loss   3.58   |   dB_loss   0.43       dA_loss    0.25\n",
            "\n",
            "Epoch:677\n",
            "gAtoB_loss   4.49       gBtoA_loss   4.75   |   dB_loss   0.4       dA_loss    0.07\n",
            "\n",
            "Epoch:678\n",
            "gAtoB_loss   4.49       gBtoA_loss   4.17   |   dB_loss   0.63       dA_loss    0.19\n",
            "\n",
            "Epoch:679\n",
            "gAtoB_loss   4.17       gBtoA_loss   3.58   |   dB_loss   0.68       dA_loss    0.06\n",
            "\n",
            "Epoch:680\n",
            "gAtoB_loss   4.76       gBtoA_loss   3.69   |   dB_loss   0.53       dA_loss    0.65\n",
            "\n",
            "Epoch:681\n",
            "gAtoB_loss   3.34       gBtoA_loss   3.99   |   dB_loss   0.39       dA_loss    0.13\n",
            "\n",
            "Epoch:682\n",
            "gAtoB_loss   3.78       gBtoA_loss   3.33   |   dB_loss   0.18       dA_loss    0.25\n",
            "\n",
            "Epoch:683\n",
            "gAtoB_loss   4.0       gBtoA_loss   3.39   |   dB_loss   0.24       dA_loss    0.39\n",
            "\n",
            "Epoch:684\n",
            "gAtoB_loss   4.2       gBtoA_loss   4.87   |   dB_loss   0.43       dA_loss    0.17\n",
            "\n",
            "Epoch:685\n",
            "gAtoB_loss   3.06       gBtoA_loss   3.39   |   dB_loss   0.21       dA_loss    0.45\n",
            "\n",
            "Epoch:686\n",
            "gAtoB_loss   4.07       gBtoA_loss   3.94   |   dB_loss   0.66       dA_loss    0.22\n",
            "\n",
            "Epoch:687\n",
            "gAtoB_loss   4.7       gBtoA_loss   4.7   |   dB_loss   0.44       dA_loss    0.19\n",
            "\n",
            "Epoch:688\n",
            "gAtoB_loss   3.42       gBtoA_loss   3.31   |   dB_loss   0.35       dA_loss    0.25\n",
            "\n",
            "Epoch:689\n",
            "gAtoB_loss   3.58       gBtoA_loss   4.08   |   dB_loss   0.23       dA_loss    0.13\n",
            "\n",
            "Epoch:690\n",
            "gAtoB_loss   4.84       gBtoA_loss   4.66   |   dB_loss   0.51       dA_loss    0.18\n",
            "\n",
            "Epoch:691\n",
            "gAtoB_loss   5.22       gBtoA_loss   4.56   |   dB_loss   0.53       dA_loss    0.54\n",
            "\n",
            "Epoch:692\n",
            "gAtoB_loss   4.22       gBtoA_loss   4.31   |   dB_loss   0.19       dA_loss    0.08\n",
            "\n",
            "Epoch:693\n",
            "gAtoB_loss   4.52       gBtoA_loss   4.63   |   dB_loss   0.23       dA_loss    0.87\n",
            "\n",
            "Epoch:694\n",
            "gAtoB_loss   4.12       gBtoA_loss   4.45   |   dB_loss   0.19       dA_loss    0.18\n",
            "\n",
            "Epoch:695\n",
            "gAtoB_loss   3.66       gBtoA_loss   3.65   |   dB_loss   0.42       dA_loss    0.24\n",
            "\n",
            "Epoch:696\n",
            "gAtoB_loss   3.49       gBtoA_loss   3.84   |   dB_loss   0.34       dA_loss    0.42\n",
            "\n",
            "Epoch:697\n",
            "gAtoB_loss   3.87       gBtoA_loss   3.36   |   dB_loss   0.06       dA_loss    0.12\n",
            "\n",
            "Epoch:698\n",
            "gAtoB_loss   4.75       gBtoA_loss   4.58   |   dB_loss   0.09       dA_loss    0.07\n",
            "\n",
            "Epoch:699\n",
            "gAtoB_loss   4.27       gBtoA_loss   3.94   |   dB_loss   0.14       dA_loss    0.34\n",
            "\n",
            "Epoch:700\n",
            "gAtoB_loss   4.3       gBtoA_loss   3.9   |   dB_loss   0.47       dA_loss    0.31\n",
            "\n",
            "\n",
            "End of training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aye38FlzGl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard \n",
        "%tensorboard --logdir \"/content/drive/My Drive/logs\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}